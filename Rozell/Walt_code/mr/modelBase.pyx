
#cython: profile=True

cimport cython
cimport numpy as np

from mr.modelBaseUnpickler import _unpickle

import sklearn.base
import sklearn.metrics

import cPickle as pickle
import cython
import inspect
import math
import numpy as np
import scipy
import sys
import time

cdef class SklearnModelBaseStats:
    """Class for debug information.  Arrays are per-patch / experiment."""

    def __str__(self):
        return ("SklearnModelBaseStats after {} samples ({} avg activity, "
                + "{} avg sqr activity, {}% used outputs, {} avg power)"
                ).format(
                    self.index,
                    np.asarray(self.activityCount).mean(),
                    np.asarray(self.activitySqr).mean(),
                    np.asarray(self.outputCount).nonzero()[0].shape[0] * 100.
                        / self.outputCount.shape[0],
                    (np.asarray(self.energy)
                        / np.maximum(1e-300, np.asarray(self.simTime))).mean())


cdef class SklearnModelBase:
    """Class that exposes an interface that is consistent with sklearn's
    interfaces.  Suitable for both supervised and unsupervised learning.

    Utilities for classes:
    * _bufferIn - An array of shape (nInputs,) for storing intermediate
            input reconstructions
    * _bufferOut - An array of shape (nOutputs,) for storing intermediate
            outputs
    * nInputs - The width of each input
    * nOutputs - The expected width out

    A default implementation requires:
    * PARAMS [list] - List of attributes that constitute the parameters of the
            model, irrespective of learned information from fit().  Anything
            specified in this list will work with set_params() and get_params().
            Should be set as (super class).PARAMS + [ 'this', 'class', 'parms' ]
    * PICKLE_VARS [list] - For cdef classes only!  Python classes are serialized
            automatically.  Lists attributes that need to be saved for pickle
            so that the model works after learning.  Should be set as
            (super class).PICKLE_VARS + [ 'this', 'class', 'parms' ]
    * UNSUPERVISED = True|False - Specifies if this is a supervised or
            unsupervised variant.
    * _init(nInputs, nOutputs) - Initializes an on-line
            algorithm for nInputs inputs and nOutputs outputs.

            May set self.nOutputs if needed, in which case it should ignore
            nOutputs.

            If UNSUPERVISED is not set at the class level, it MUST be set after
            this function is called.
    * _partial_fit(x, y) - Aggregates new data x shape (nInputs)
            into the learned parameters.  x and y have already been type
            checked.  y will always be None if UNSUPERVISED is True.
    * _predict(x, y) - fills y with the outputs for x
    * _reconstruct(y, r) - Fills r (which is an np.array of dtype float with
            length self.nInputs) with the reconstruction of inputs for the
            given output vector y (or the highest likelihood inputs for the
            given outputs).

    If an algorithm is offline (access to all training data), then the deriving
    class should override fit instead of _partial_fit.  Be sure to call
    self._checkDatasets(X, y), and self.init(). Everything else should be the
    same.
    """

    # Value for debug kwarg to functions that means do NOT reset the debug
    # table, but do debug.
    DEBUG_CONTINUE = "continue"
    PARAMS = [ 'nOutputs' ]
    # Variables that AREN'T in PARAMS or __dict__ that need to be saved on
    # pickle.  Doesn't apply to python classes, only cdef classes!
    PICKLE_VARS = [ '_bufferIn', '_bufferOut', 'nInputs', '_isInit', 't_' ]
    UNSUPERVISED = None

    property nOutputsConvolved:
        """The number of outputs that are convolved; this is used purely for
        visualizing a network."""
        def __get__(self):
            return self.nOutputs


    def __init__(self, **kwargs):
        """Initializes the base UnsupervisedPredictor.  Requires knowledge of
        how many outputs are generated by this layer."""
        self._isInit = False
        self._debug = False
        self.debugInfo_ = SklearnModelBaseStats()
        self.set_params(**kwargs)


    def __reduce_ex__(self, protocol):
        """Pickle protocol: return (callable, callableArgs, __dict__ states)"""
        d = {}
        for k in self.PICKLE_VARS:
            d[k] = getattr(self, k, None)
        if hasattr(self, '__dict__'):
            for k, v in self.__dict__.iteritems():
                if k in self.PICKLE_VARS or k in self.PARAMS:
                    continue
                d[k] = v

        # Convert memoryviews to arrays, which can be pickled
        for k, v in d.iteritems():
            vt = type(v)
            if getattr(vt, '__name__', None) == '_memoryviewslice':
                d[k] = v.base

        return (_unpickle, (self.__class__, self.get_params(),), d)


    def __setstate__(self, state):
        for k, v in state.iteritems():
            setattr(self, k, v)


    @classmethod
    def _get_param_names(cls):
        """Stolen from scipy.base.BaseEstimator."""
        return sorted(cls.PARAMS)


    def get_params(self, deep=True):
        out = dict()
        for key in self._get_param_names():
            value = getattr(self, key, None)

            if deep:
                if hasattr(value, 'get_params'):
                    deep_items = value.get_params().items()
                    out.update((key + '__' + k, val) for k, val in deep_items)
                    value = {'__estimator__': type(value)}
                elif isinstance(value, list):
                    # We have a list as a parameter, see if its members have
                    # get_value
                    valueOut = []
                    for i, v in enumerate(value):
                        if hasattr(v, 'get_params'):
                            deep_items = v.get_params().items()
                            out.update(('{}__{}__{}'.format(key, i, k), sv)
                                    for k, sv in deep_items)
                            valueOut.append({'__estimator__': type(v)})
                        else:
                            valueOut.append(v)
                    value = valueOut
            out[key] = value
        return out


    def set_params(self, **params):
        if not params:
            return self
        valid_params = self.get_params(deep = False)
        for key, value in sorted(params.iteritems()):
            split = key.split('__', 1)
            if len(split) > 1:
                name, sub_name = split
                if not name in valid_params:
                    raise ValueError("{} in {}?".format(name, self))
                sub_object = getattr(self, name)
                if hasattr(sub_object, 'get_params'):
                    sub_object.set_params(**{sub_name:value})
                elif isinstance(sub_object, list):
                    index, value_name = sub_name.split('__', 1)
                    sub_object[int(index)].set_params(**{value_name:value})
                else:
                    raise ValueError("Param type not recognized!")
            else:
                if not key in valid_params:
                    raise ValueError("{} in {}?".format(key, self))

                if (isinstance(value, dict) and len(value) == 1
                        and '__estimator__' in value):
                    value = value['__estimator__']()
                elif isinstance(value, list):
                    valueOut = []
                    for v in value:
                        if (isinstance(v, dict) and len(v) == 1
                                and '__estimator__' in v):
                            valueOut.append(v['__estimator__']())
                        else:
                            valueOut.append(v)
                    value = valueOut
                setattr(self, key, value)
        return self


    def fit(self, X, y=None, maxIters=0, solveTime=0.0, debug = False):
        """Initializes this predictor's learned values so that they fit the
        dimensionality of X, and trains the predictor on X.

        maxIters [int, default 0] - The maximum number of iterations to train
                on.

        solveTime - For smaller datasets, it probably makes sense to run fit()
                across the dataset multiple times by default.  solveTime
                specifies, in seconds, the amount of time that an
                UnsupervisedPredictor is allowed to use on trying to better
                output by multiple partial_fits.
        """
        self._checkDatasets(X, y, isInit=True)

        cdef double score, oldScore, tdur, tstart
        self.init(len(X[0]), None if y is None else len(y[0]))

        self._resetDebug(debug, 0)
        if debug:
            # We want to record all fit iterations, so set it to DEBUG_CONTINUE
            # to preserve each fit's records.
            debug = self.DEBUG_CONTINUE

        tstart = time.time()
        self.partial_fit(X, y, debug = debug)
        if maxIters > 0 and self.fitIters_ >= maxIters:
            return
        tdur = time.time() - tstart

        score = self.score(X, y)
        while solveTime <= 0 or time.time() + tdur < tstart + solveTime:
            self.partial_fit(X, y, debug = debug)
            if maxIters > 0 and self.fitIters_ >= maxIters:
                return

            oldScore = score
            score = self.score(X, y)
            if self.UNSUPERVISED:
                # We try to minimize mean-squared-error
                if (score - oldScore) / oldScore > -0.01:
                    # Additional runs don't help
                    break
            else:
                # We try to maximize r^2 of prediction
                if (score - oldScore) / abs(oldScore) < 0.01:
                    break


    def fit_predict(self, X, y=None):
        self.fit(X, y)
        return self.predict(X)


    def init(self, nInputs, nOutputs):
        """nInputs and nOutputs comes from data, so init()
        comes after constructor.  Note that nInputs and nOutputs as passed to
        this function do not necessarily override the actual nInputs and
        nOutputs.

        Calls self._init(self.nInputs, self.nOutputs)."""
        self._isInit = True
        self.nInputs = nInputs
        if self.nOutputs == 0:
            if nOutputs is None:
                raise ValueError("init() cannot be called with nOutputs == "
                        "None if nOutputs is not specified in the constructor")
            self.nOutputs = nOutputs
        self._init(self.nInputs, self.nOutputs)
        if self.UNSUPERVISED is None:
            raise ValueError("Class {} must have UNSUPERVISED defined as True "
                    "or False after _init()".format(self.__class__.__name__))
        if self.nOutputs == 0:
            raise ValueError("self.nOutputs must be set in _init if not "
                    "before")
        self._bufferIn = np.zeros(self.nInputs, dtype = float)
        self._bufferOut = np.zeros(self.nOutputs, dtype = float)

        # Learning scalars - final scalar is:
        # 1.0 / pow(t_, power_t)
        self.t_ = 1.0

        self.fitIters_ = 0


    def partial_fit(self, X, y=None, debug = False):
        self._checkDatasets(X, y)

        if not self._isInit:
            # First time initialization
            self.init(len(X[0]), len(y[0]) if y is not None else None)

        self.fitIters_ += 1

        cdef int i, j
        cdef double t
        cdef double[:] _bufferOut = self._bufferOut
        cdef double noutInv = 1. / self.nOutputs

        self._resetDebug(debug, len(X))

        for i in range(len(X)):
            self._partial_fit(X[i], None if y is None else y[i])
            self.t_ += 1.

            if self._debug:
                # Assumes self._bufferOut was populated!
                for j in range(self.nOutputs):
                    t = _bufferOut[j]
                    if abs(t) >= 0.01:
                        self.debugInfo_.activityCount[
                                self.debugInfo_.index] += noutInv
                        self.debugInfo_.outputCount[j] += 1
                    self.debugInfo_.activitySqr[self.debugInfo_.index] += t*t
                    self.debugInfo_.outputSqr[j] += t*t
                self.debugInfo_.index += 1


    def predict(self, X, debug = False):
        """Given X, calculate our output values Y (which can be thought of as
        the strengths of different clusters / classification elements)"""
        self._checkDatasets(X, None, True)

        if not self._isInit:
            raise ValueError("{} must be initialized before predict()".format(
                    self))

        cdef double[:, :] Y = np.zeros((len(X), self.nOutputs), dtype = float)
        cdef int i, j
        cdef double t
        cdef double noutInv = 1. / self.nOutputs

        self._resetDebug(debug, len(X))

        for i in range(len(X)):
            self._predict(X[i], Y[i])

            if self._debug:
                # Assumes self._bufferOut was populated!
                for j in range(self.nOutputs):
                    t = Y[i, j]
                    if abs(t) >= 0.01:
                        self.debugInfo_.activityCount[
                                self.debugInfo_.index] += noutInv
                        self.debugInfo_.outputCount[j] += 1
                    self.debugInfo_.activitySqr[self.debugInfo_.index] += t*t
                    self.debugInfo_.outputSqr[j] += t*t
                self.debugInfo_.index += 1
        return np.asarray(Y)


    def reconstruct(self, X):
        """Reconstruct each member of X and return the reconstructions"""
        self._checkDatasets(X, None, True)
        if not self._isInit:
            raise ValueError("{} must be initialized before predict()".format(
                    self))
        cdef int i
        cdef double[:, :] Y = np.zeros((len(X), self.nInputs), dtype = float)
        for i in range(len(X)):
            self._predict(X[i], self._bufferOut)
            self._reconstruct(self._bufferOut, Y[i])
        return Y


    def reconstructFromPredict(self, y):
        """Reconstruct each member of the original X given a result from
        predict()."""
        self._checkDatasets(y, None, True)
        if not self._isInit:
            raise ValueError("{} must be initialized before predict()".format(
                    self))
        cdef int i
        cdef double[:, :] X = np.zeros((len(y), self.nInputs), dtype=float)
        for i in range(len(y)):
            self._reconstruct(y[i], X[i])
        return X


    def score(self, X, y=None, debug = False):
        """Returns the mean-squared-error for the distance from the
        reconstruction of X to X, divided by the number of elements in X.
        """
        self._checkDatasets(X, y)

        cdef int i, j
        cdef double[:, :] pX
        if self.UNSUPERVISED and y is not None:
            raise ValueError("Cannot give y with UNSUPERVISED")

        pX = self.predict(X, debug = debug)

        if not self.UNSUPERVISED:
            # Return the r2 error
            return sklearn.metrics.r2_score(y, pX)

        mse = 0.0
        mseDivisor = len(X) * self.nInputs
        cdef double[:] vX
        for i in range(len(X)):
            self._reconstruct(pX[i, :], self._bufferIn)
            vX = X[i]
            for j in range(self.nInputs):
                mse += (self._bufferIn[j] - vX[j]) ** 2
        return mse / mseDivisor


    def visualize(self, params, path = None, inputs = None):
        """Dumps an image at path, based on the visual params (width, height,
        channels).  Essentially, gives the input map that is the most likely
        estimator to produce a single output.

        inputs [None] - Inputs suitable for self.predict.  Determines the
                inputs to visualize
        """
        cdef int i, j
        cdef double[:] outputVals, _bufferIn = self._bufferIn

        w, h, channels = params
        if w * h * channels != self.nInputs:
            raise ValueError("Bad visualization params")

        def split(layerX, layerY, j):
            """returns x, y, c for x coord, y coord, and color channel of
            input j."""
            c = j % channels
            j //= channels
            ox = layerX + (j % w)
            oy = layerY + (j // w)
            return (ox, oy, c)

        imh = h
        imw = w * self.nOutputs
        outputVals = np.ones(self.nOutputs, dtype = float)
        if inputs is not None:
            imh += h
        imdata = np.zeros((imh, imw, channels), dtype = np.uint8)

        if inputs is not None:
            self._predict(np.asarray(inputs), outputVals)
            # Print the reconstruction
            self._reconstruct(outputVals, _bufferIn)
            for j in range(self.nInputs):
                ox, oy, c = split(w, h, j)
                imdata[oy, ox, c] = int(
                        255 * max(0.0, min(1.0, _bufferIn[j])))

        for i in range(self.nOutputs):
            self._bufferOut[:] = 0
            self._bufferOut[i] = 1.0 * outputVals[i]
            self._reconstruct(self._bufferOut, self._bufferIn)
            for j in range(self.nInputs):
                ox, oy, c = split(i * w, 0, j)
                imdata[oy, ox, c] = int(255 * max(0.0, min(1.0,
                        self._bufferIn[j])))

        if inputs is not None:
            # Print the inputs
            for j in range(self.nInputs):
                ox, oy, c = split(0, h, j)
                imdata[oy, ox, c] = int(255 * max(0.0, min(1.0, inputs[j])))

        if channels == 1:
            imdata = imdata[:, :, 0]
        scipy.misc.imsave(path, imdata)


    def _checkDatasets(self, X, y, noYIsOk = False, isInit=False):
        """Checks input / output arrays X and y to make sure they have two
        dimensions and can be indexed via [].

        Raises a ValueError if the parameter is invalid.

        :param isInit: If True, then self.nInputs and self.nOutputs will be set
                after this method is called, and those checks will be skipped.
        """
        if not noYIsOk:
            if self.UNSUPERVISED and y is not None:
                raise ValueError("Unsupervised, cannot have y!")
            elif not self.UNSUPERVISED and y is None:
                raise ValueError("Supervised, must have y!")
        elif y is not None:
            raise ValueError("When noYIsOk, there should be no y!")

        if X is None:
            raise ValueError("X must be specified")

        for name, obj in [ ("X", X), ("y", y) ]:
            if obj is None:
                # None checking is done above
                continue

            for attr in [ '__len__', '__getitem__' ]:
                if not hasattr(obj, attr):
                    raise ValueError("{} does not support {}}".format(name,
                            attr))
            if not hasattr(obj[0], '__len__'):
                raise ValueError("{} does not look like a 2-d array".format(
                        name))

        if not isInit:
            if len(X[0]) != self.nInputs:
                raise ValueError("Wrong input dimension: {}, not {}".format(
                        len(X[0]), self.nInputs))


    cpdef _init(self, int nInputs, int nOutputs):
        raise NotImplementedError()


    cpdef _partial_fit(self, double[:] x, double[:] y):
        raise NotImplementedError()


    cpdef _predict(self, double[:] x, double[:] y):
        raise NotImplementedError()


    cpdef _reconstruct(self, double[:] y, double[:] r):
        raise NotImplementedError()


    cpdef _resetDebug(self, debug, int lenX):
        """We're about to do some operations on input array X.  Set up the
        debugInfo_ struct if self.debug is set.
        """
        cdef double[:] new, tmp

        if not debug:
            self._debug = False
            return

        self._debug = True
        s1 = [ 'activityCount', 'activitySqr', 'energy', 'simTime' ]
        if debug != self.DEBUG_CONTINUE:
            self.debugInfo_.index = 0
            for aname in s1:
                setattr(self.debugInfo_, aname, np.zeros(lenX))

            # Reset output stuff
            self.debugInfo_.outputCount = np.zeros(self.nOutputs)
            self.debugInfo_.outputSqr = np.zeros(self.nOutputs)
        else:
            tmp = self.debugInfo_.activity
            for aname in s1:
                tmp = getattr(self.debugInfo_, aname)
                new = np.zeros(len(tmp) + lenX)
                new[:len(tmp)] = tmp
                setattr(self.debugInfo_, aname, new)
